{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import savefig\n",
    "import cv2\n",
    "import glob as gl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "\n",
    "path_hd = 'dataset/train/train_hd/*.jpg'\n",
    "path_messy = 'dataset/train/train_messy/*.jpg'\n",
    "\n",
    "paths_list_messy = [path_messy]\n",
    "paths_list_hd = [path_hd]\n",
    "\n",
    "for path in paths_list_messy:\n",
    "    filenames = gl.glob(path)\n",
    "    for filename in filenames:\n",
    "       \n",
    "        messy_img = cv2.imread(filename)\n",
    "        messy_img = cv2.resize(messy_img, (128, 128))\n",
    "    \n",
    "        # mkdir if folder not exist\n",
    "        cv2.imwrite(\"messy_images/messy_\" +str(count) +\".jpg\", messy_img)\n",
    "        count += 1\n",
    "\n",
    "count = 1\n",
    "for path in paths_list_hd:\n",
    "    filenames = gl.glob(path)\n",
    "    for filename in filenames:\n",
    "       \n",
    "        image = cv2.imread(filename)\n",
    "        image = cv2.resize(image, (128, 128))\n",
    "    \n",
    "        cv2.imwrite(\"hd_images/hd_\" +str(count) +\".jpg\", image)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = gl.glob(\"dataset/test/test_hd/*.jpg\")\n",
    "\n",
    "count = 1\n",
    "for file in filenames:\n",
    "    img = cv2.imread(file)\n",
    "    img = cv2.resize(img, (128, 128))\n",
    "    cv2.imwrite('actual_hd_test/hd_' + str(count) + '.jpg', img)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 426\n",
    "\n",
    "dataset = []\n",
    "\n",
    "for i in range(1, num_images + 1):\n",
    "    #print(i)\n",
    "    img = cv2.imread(\"hd_images/hd_\" +str(i) + \".jpg\")\n",
    "    #print(img)\n",
    "    dataset.append(np.array(img))\n",
    "\n",
    "dataset_source = np.asarray(dataset)\n",
    "print(dataset_source.shape)\n",
    "\n",
    "dataset_tar = []\n",
    "\n",
    "for i in range(1, num_images + 1):\n",
    "    img = cv2.imread(\"messy_images/messy_\" +str(i) + \".jpg\")\n",
    "    #print(img)\n",
    "    dataset_tar.append(np.array(img))\n",
    "\n",
    "dataset_target = np.asarray(dataset_tar)\n",
    "print(dataset_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder(inputs):  # Undercomplete Autoencoder\n",
    "\n",
    "    # Encoder\n",
    "    net = tf.layers.conv2d(inputs, 128, 2, activation=tf.nn.relu)\n",
    "    net = tf.layers.max_pooling2d(net, 2, 2, padding='same')\n",
    "    print(net.shape)\n",
    "\n",
    "    # Decoder\n",
    "    net = tf.image.resize_nearest_neighbor(net, tf.constant([129, 129]))\n",
    "    net = tf.layers.conv2d(net, 1, 2, activation=None, name='outputOfAuto')\n",
    "\n",
    "    print(net.shape)\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The “None” represents that the batch size would be determined at the runtime.\n",
    "ae_inputs = tf.placeholder(tf.float32, (None, 128, 128, 3), name='inputToAuto')\n",
    "ae_target = tf.placeholder(tf.float32, (None, 128, 128, 3))\n",
    "\n",
    "ae_outputs = autoencoder(ae_inputs)\n",
    "lr = 0.001  # learning rate \n",
    "\n",
    "loss = tf.reduce_mean(tf.square(ae_outputs - ae_target))\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "# Intialize the network \n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "epoch_num = 1\n",
    "\n",
    "saving_path = 'autoencoder_messy_to_hd/SavedModel/AutoencoderMessyToHd.ckpt'\n",
    "\n",
    "saver_ = tf.train.Saver(max_to_keep=3)\n",
    "\n",
    "batch_img = dataset_source[0:batch_size]\n",
    "batch_out = dataset_target[0:batch_size]\n",
    "\n",
    "num_batches = num_images // batch_size\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for ep in range(epoch_num):\n",
    "    batch_size = 0\n",
    "    for batch_n in range(num_batches):  # batches loop\n",
    "\n",
    "        _, c = sess.run([train_op, loss], feed_dict={ae_inputs: batch_img, ae_target: batch_out})\n",
    "        print(\"Epoch: {} - cost = {:.5f}\".format((ep + 1), c))\n",
    "\n",
    "        batch_img = dataset_source[batch_size: batch_size + 32]\n",
    "        batch_out = dataset_target[batch_size: batch_size + 32]\n",
    "\n",
    "        batch_size += 32\n",
    "\n",
    "    saver_.save(sess, saving_path, global_step=ep)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "saver.restore(sess, 'autoencoder_messy_to_hd/SavedModel/AutoencoderMessyToHd.ckpt-49')\n",
    "\n",
    "filenames = gl.glob('actual_hd_test/*.jpg')\n",
    "\n",
    "test_data = []\n",
    "for file in filenames[0:100]:\n",
    "    test_data.append(np.array(cv2.imread(file)))\n",
    "\n",
    "test_dataset = np.asarray(test_data)\n",
    "print(test_dataset.shape)\n",
    "\n",
    "# Running the test data on the autoencoder\n",
    "batch_imgs = test_dataset\n",
    "hd_imgs = sess.run(ae_outputs, feed_dict = {ae_inputs: batch_imgs})\n",
    "\n",
    "for i in range(hd_imgs.shape[0]):\n",
    "    cv2.imwrite('gen_hd_images/gen_hd_' +str(i) +'.jpg', hd_imgs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
